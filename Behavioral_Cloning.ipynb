{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b09a0b12",
      "metadata": {
        "id": "b09a0b12"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leeoos/miniconda3/envs/hum_rl/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4bb18b9730>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from os.path import join, exists\n",
        "from os import mkdir, unlink, listdir, getpid, remove\n",
        "#from .autonotebook import tqdm as notebook_tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from statistics import mean\n",
        "import math\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ce03945f",
      "metadata": {
        "id": "ce03945f"
      },
      "outputs": [],
      "source": [
        "##dataset class\n",
        "from torch.utils.data.dataset import Dataset, random_split\n",
        "\n",
        "class ExpertDataSet(Dataset):\n",
        "\n",
        "    def __init__(self, expert_observations, expert_actions):\n",
        "        self.observations = torch.from_numpy(expert_observations)\n",
        "        self.actions = self.preprocess_data(torch.from_numpy(expert_actions))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # return (self.observations[index], self.actions[index])\n",
        "        normalized_observations = 2 * ((self.observations[idx] - self.observations.min()) / (self.observations.max() - self.observations.min())) - 1\n",
        "        # normalized_actions = 2 * ((self.actions[idx] - self.actions.min()) / (self.actions.max() - self.actions.min())) - 1\n",
        "        normalized_data = (normalized_observations, self.actions[idx])\n",
        "        return normalized_data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.observations)\n",
        "    \n",
        "    def preprocess_data(self, data, clip_value=1e38):\n",
        "        # Clip values to a maximum and minimum range\n",
        "        data = torch.clamp(data, min=-clip_value, max=clip_value)\n",
        "        \n",
        "        # Convert to float\n",
        "        return data.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "11af5407",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11af5407",
        "outputId": "bc497a92-1701-4426-c1cd-4fb74f7545cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expert actions len: 8000\n",
            "Expert observations len: 8000\n",
            "Discarded data\n",
            "Discarded form np: 0\n",
            "Discarded form torch: 0\n",
            "Shapes:\n",
            "torch.Size([36])\n",
            "torch.Size([196])\n"
          ]
        }
      ],
      "source": [
        "expert_observations = np.load('./expert-observations.npy', allow_pickle=True)\n",
        "expert_actions = np.load('./expert-actions.npy', allow_pickle=True)\n",
        "\n",
        "count_discarded_numpy = 0\n",
        "count_discarded = 0\n",
        "\n",
        "\n",
        "new_exp_action = expert_actions\n",
        "\n",
        "list_of_index_to_drop = []\n",
        "for i, a in enumerate(expert_actions):\n",
        "  if (a > 1e2).any() or (a > 1e2).any():\n",
        "  # if not np.isfinite(a).all(): \n",
        "    list_of_index_to_drop.append(i)\n",
        "    print(i)\n",
        "    print(a)\n",
        "    count_discarded_numpy+=1\n",
        "    # break\n",
        "\n",
        "\n",
        "print(\"Expert actions len: {}\".format(len(expert_actions)))\n",
        "print(\"Expert observations len: {}\".format(len(expert_observations)))\n",
        "\n",
        "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(expert_dataset)):\n",
        "  a = expert_dataset.__getitem__(i)[1]\n",
        "  # print(a.max())\n",
        "  # print(a.min())\n",
        "  if (a > 1e2).any() or (a < -1e2).any() :\n",
        "  # if not torch.isfinite(a).any():\n",
        "    count_discarded += 1\n",
        "    print(a)\n",
        "\n",
        "\n",
        "print(\"Discarded data\")\n",
        "print(\"Discarded form np: {}\".format(count_discarded_numpy))\n",
        "print(\"Discarded form torch: {}\".format(count_discarded))\n",
        "\n",
        "#split in 80% training and 20%test\n",
        "batch_size = 64\n",
        "train_prop = 0.8\n",
        "train_size = int(train_prop * len(expert_dataset))\n",
        "test_size = len(expert_dataset) - train_size\n",
        "train_expert_dataset, test_expert_dataset = random_split(expert_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(  dataset=train_expert_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(  dataset=test_expert_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(train_loader.dataset.__getitem__(0)[1].shape)\n",
        "print(train_loader.dataset.__getitem__(0)[0].shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94873ce1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Policy agent\n",
        "\n",
        "class BCAgent(nn.Module):\n",
        "\n",
        "  def __init__(self, obs_space, action_space) -> None:\n",
        "    self.name = 'Behavioral-Cloning-Agent'\n",
        "\n",
        "    self.n_inputs = obs_space\n",
        "    self.n_outputs = action_space\n",
        "\n",
        "    # Policy Network\n",
        "    self.fc1 = nn.Linear(self.n_inputs)\n",
        "    self.bn1 = nn.BatchNorm1d(16)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(16, self.n_outputs)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.fc1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    return out\n",
        "  \n",
        "  def load_parameters(self, dir): \n",
        "    if exists(dir+self.name.lower()+'.pt'):\n",
        "        print(\"Loading model \"+self.name+\" state parameters\")\n",
        "        self.load_state_dict(torch.load(dir+self.name.lower()+'.pt', map_location=self.device))\n",
        "        return self\n",
        "    else:\n",
        "        print(\"Error no model \"+self.name.lower()+\" found!\")\n",
        "        exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09e78876",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train functions\n",
        "\n",
        "def train(\n",
        "        policy,\n",
        "        train_epochs,\n",
        "        eval_epochs,\n",
        "        train_loader, \n",
        "        test_loader,\n",
        "        optimizer,\n",
        "        loss_criterion\n",
        "    ):\n",
        "\n",
        "    policy.train()\n",
        "    policy.to(device)\n",
        "    \n",
        "    loss = 0\n",
        "    epoch_loss = 0\n",
        "    unused_val = 0\n",
        "\n",
        "    for epoch in range(train_epochs):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "            obs, expert_action = data.to(device), target.to(device)\n",
        "            obs = obs.float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            student_action = policy(obs)\n",
        "            expert_action = expert_action.float()\n",
        "\n",
        "            loss = loss_criterion(student_action, expert_action)\n",
        "            # loss.register_hook(lambda grad: print(grad))\n",
        "            loss.backward()\n",
        "            # print(\"Loss: {}\".format(loss.item()))\n",
        "            \n",
        "            \n",
        "            if not loss.item() == torch.inf: \n",
        "                epoch_loss += loss.item()\n",
        "                optimizer.step()\n",
        "\n",
        "            else:\n",
        "                unused_val += 1\n",
        "                # print(\"### BATCH {} ###\".format(batch_idx))\n",
        "                # print(f'obs -> {obs}')\n",
        "                print(\"\\n______________________________________________________________________________\")\n",
        "                print(f'expert_action -> {expert_action}')\n",
        "                print(\"\\n______________________________________________________________________________\")\n",
        "                print(f'student_action -> {student_action}')\n",
        "                print(\"\\n______________________________________________________________________________\")\n",
        "                return expert_action,student_action\n",
        "\n",
        "            res = print_gradients()\n",
        "            \n",
        "            # print(\"###############################################################################\\n\")\n",
        "\n",
        "            if torch.isnan(student_action).any(): \n",
        "                print('e successo')\n",
        "                break\n",
        "            if res == 1: \n",
        "                print(\"\\n______________________________________________________________________________\")\n",
        "                print(student_action.shape)\n",
        "                for i, ea in enumerate(expert_action):\n",
        "                    if not np.isfinite(ea).all():\n",
        "                        print(i+64)\n",
        "                        print(f'expert_action -> {ea}')\n",
        "\n",
        "                print(\"\\n______________________________________________________________________________\")\n",
        "                print(f'Max expert_action -> {expert_action.max()}')\n",
        "                print(f'Min expert_action -> {expert_action.min()}')\n",
        "                print(f'Max student_action -> {student_action.max()}')\n",
        "                print(f'Min student_action -> {student_action.min()}')\n",
        "                break\n",
        "            \n",
        "            # print(\"Student actions: {}\".format(student_action.shape))\n",
        "            # print(\"Expert actions: {}\".format(expert_action.shape))\n",
        "        \n",
        "        \n",
        "        # compute accuracy\n",
        "        print(\"Epoch {}\".format(epoch))\n",
        "        print(\"Train Loss: {}\".format(epoch_loss/(batch_idx+1)))\n",
        "        validation(test_loader, num_epochs=eval_epochs)\n",
        "        print(\"Unused Loss: {}\".format(unused_val))\n",
        "        epoch_loss = 0\n",
        "        unused_val = 0\n",
        "        print(\"###############################################################################\\n\")\n",
        "\n",
        "\n",
        "def validation(self, loader, num_epochs):\n",
        "    self.policy.eval()\n",
        "    epoch_loss = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, (data, target) in enumerate(loader):\n",
        "            obs, expert_action = data.to(device), target.to(device)\n",
        "            obs = obs.float()\n",
        "            student_action = self.policy(obs)\n",
        "            loss = self.loss_criterion(student_action, expert_action)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    print(\"Validation Loss: {}\".format(epoch_loss/(batch_idx+1)))\n",
        "\n",
        "def print_gradients(self):\n",
        "    for name, param in self.policy.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if torch.isnan(param.grad).any(): \n",
        "                return 1#break\n",
        "            # print(f\"Gradient of {name}: {param.grad}\")\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d8a59c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train module\n",
        "\n",
        "policy = BCAgent()\n",
        "loss_criterion = nn.MSELoss()\n",
        "optimizer =  optim.Adam(policy.parameters(), lr=1e-2)\n",
        "eval_epochs = 5\n",
        "\n",
        "train(policy, \n",
        "      train_epochs=100, \n",
        "      eval_epochs=5, \n",
        "      train_loader=train_loader, \n",
        "      test_loader=test_loader,\n",
        "      optimizer=optimizer,\n",
        "      loss_criterion=loss_criterion\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82745485",
      "metadata": {},
      "outputs": [],
      "source": [
        "# save trained model\n",
        "\n",
        "torch.save(policy.state_dict, './checkpoints/'+policy.name.lower()+'.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4c7a5b24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c7a5b24",
        "outputId": "88c86e15-4dea-434a-97c2-d976be5ce2a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "use_cuda:  False\n"
          ]
        }
      ],
      "source": [
        "###### Define student agent\n",
        "\n",
        "\n",
        "class StudentAgent:\n",
        "    def __init__(self, train_loader, test_loader, learning_rate, threshold):\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "\n",
        "        n_inputs = train_loader.dataset.__getitem__(0)[0].shape[0] # dimension of observation space (of state = 196)\n",
        "        n_outputs = train_loader.dataset.__getitem__(0)[1].shape[0] # dimension of actions space (of outptu = 36)\n",
        "\n",
        "        #simple layer\n",
        "        self.policy = nn.Sequential(\n",
        "            # nn.BatchNorm1d(n_inputs),\n",
        "            nn.Linear(n_inputs, 16), \n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, n_outputs),\n",
        "            # nn.BatchNorm1d(n_outputs)\n",
        "            # nn.Softmax(dim=-1)\n",
        "            )\n",
        "\n",
        "        print(\"policy net: \", self.policy)\n",
        "\n",
        "        # self.loss_criterion = nn.HuberLoss() #nn.MSELoss()\n",
        "        self.loss_criterion = nn.MSELoss()\n",
        "\n",
        "        self.optimizer =  optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
        "        self.num_eval_episodes = 5\n",
        "        self.accuracy_threshold = threshold\n",
        "\n",
        "    def print_gradients(self):\n",
        "        for name, param in self.policy.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                if torch.isnan(param.grad).any(): \n",
        "                    return 1#break\n",
        "                # print(f\"Gradient of {name}: {param.grad}\")\n",
        "        return 0\n",
        "\n",
        "    def train(self, num_epochs):\n",
        "        self.policy.train()\n",
        "        self.policy.to(device)\n",
        "        \n",
        "        loss = 0\n",
        "        epoch_loss = 0\n",
        "        unused_val = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            for batch_idx, (data, target) in enumerate(self.train_loader):\n",
        "                obs, expert_action = data.to(device), target.to(device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                obs = obs.float()\n",
        "\n",
        "                student_action = self.policy(obs)\n",
        "                expert_action = expert_action.float()\n",
        "\n",
        "                loss = self.loss_criterion(student_action, expert_action)\n",
        "                # loss.register_hook(lambda grad: print(grad))\n",
        "                loss.backward()\n",
        "                # print(\"Loss: {}\".format(loss.item()))\n",
        "                \n",
        "                \n",
        "                if not loss.item() == torch.inf: \n",
        "                    epoch_loss += loss.item()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                else:\n",
        "                    unused_val += 1\n",
        "                    # print(\"### BATCH {} ###\".format(batch_idx))\n",
        "                    # print(f'obs -> {obs}')\n",
        "                    print(\"\\n______________________________________________________________________________\")\n",
        "                    print(f'expert_action -> {expert_action}')\n",
        "                    print(\"\\n______________________________________________________________________________\")\n",
        "                    print(f'student_action -> {student_action}')\n",
        "                    print(\"\\n______________________________________________________________________________\")\n",
        "                    return expert_action,student_action\n",
        "\n",
        "                res = self.print_gradients()\n",
        "                \n",
        "                # print(\"###############################################################################\\n\")\n",
        "\n",
        "                if torch.isnan(student_action).any(): \n",
        "                    print('e successo')\n",
        "                    break\n",
        "                if res == 1: \n",
        "                    print(\"\\n______________________________________________________________________________\")\n",
        "                    print(student_action.shape)\n",
        "                    for i, ea in enumerate(expert_action):\n",
        "                        if not np.isfinite(ea).all():\n",
        "                            print(i+64)\n",
        "                            print(f'expert_action -> {ea}')\n",
        "\n",
        "                    print(\"\\n______________________________________________________________________________\")\n",
        "                    print(f'Max expert_action -> {expert_action.max()}')\n",
        "                    print(f'Min expert_action -> {expert_action.min()}')\n",
        "                    print(f'Max student_action -> {student_action.max()}')\n",
        "                    print(f'Min student_action -> {student_action.min()}')\n",
        "                    break\n",
        "                \n",
        "                # print(\"Student actions: {}\".format(student_action.shape))\n",
        "                # print(\"Expert actions: {}\".format(expert_action.shape))\n",
        "            \n",
        "            \n",
        "            # compute accuracy\n",
        "            train_acc = self.compute_accuracy(self.train_loader)\n",
        "            test_acc = self.compute_accuracy(self.test_loader)\n",
        "            # print(\"Epoch {}:\\ttrain accuracy: {}\\ttest accuracy: {}\".format(epoch, train_acc, test_acc))\n",
        "            print(\"Epoch {}\".format(epoch))\n",
        "            print(\"Train Loss: {}\".format(epoch_loss/(batch_idx+1)))\n",
        "            self.validation(test_loader, num_epochs=self.num_eval_episodes)\n",
        "            print(\"Unused Loss: {}\".format(unused_val))\n",
        "            epoch_loss = 0\n",
        "            unused_val = 0\n",
        "            # if train_acc >80. and test_acc>80.: return\n",
        "            print(\"###############################################################################\\n\")\n",
        "\n",
        "\n",
        "    def validation(self, loader, num_epochs):\n",
        "        self.policy.eval()\n",
        "        epoch_loss = 0\n",
        "        for epoch in range(num_epochs):\n",
        "            for batch_idx, (data, target) in enumerate(loader):\n",
        "                obs, expert_action = data.to(device), target.to(device)\n",
        "                obs = obs.float()\n",
        "                student_action = self.policy(obs)\n",
        "                loss = self.loss_criterion(student_action, expert_action)\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "        print(\"Validation Loss: {}\".format(epoch_loss/(batch_idx+1)))\n",
        "\n",
        "\n",
        "\n",
        "    def compute_accuracy(self, loader):\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        self.policy.eval()\n",
        "        test_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in loader:\n",
        "                obs, expert_action = data.to(device), target.to(device)\n",
        "                obs = obs.float()\n",
        "                student_action = self.policy(obs)\n",
        "\n",
        "                total += student_action.size(0)\n",
        "\n",
        "                # print(\"Total: {}\".format(total))\n",
        "                # print(\"\\n______________________________________________________________________________\")\n",
        "                # print(f'expert_action -> {expert_action}')\n",
        "                # print(\"\\n______________________________________________________________________________\")\n",
        "                # print(f'student_action -> {student_action}')\n",
        "                # print(\"\\n______________________________________________________________________________\")\n",
        "                # similarity = math.isclose(student_action.any(), expert_action.any(), rel_tol=1e-5)\n",
        "\n",
        "                # difference = (expert_action - student_action).abs()\n",
        "                similarity= torch.abs(student_action - expert_action)\n",
        "                similarity = torch.sum(similarity,dim=1)/36\n",
        "                manual_mse = torch.mean(torch.square(student_action) - torch.square(expert_action),dim=0)\n",
        "\n",
        "                correct  += (similarity < self.accuracy_threshold).sum().item()\n",
        "                # print(f'similarity -> {similarity.shape}') # 64\n",
        "                # print(f'similarity -> {similarity.mean()}')\n",
        "                # print(f'similarity -> {similarity.shape}')\n",
        "\n",
        "                \n",
        "                # print(f'mse -> {manual_mse}')\n",
        "                # print(f'shape -> {manual_mse.shape}')\n",
        "\n",
        "\n",
        "                # print(f'correct -> {correct}')\n",
        "                # correct += int(similarity/total)\n",
        "                # correct += sum(student_action==expert_action).item()\n",
        "                \n",
        "\n",
        "        # print(f'correct -> {correct}')\n",
        "        accuracy = 100. * correct/(total)\n",
        "\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "86cbb1ec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "policy net:  Sequential(\n",
            "  (0): Linear(in_features=196, out_features=16, bias=True)\n",
            "  (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU()\n",
            "  (3): Linear(in_features=16, out_features=36, bias=True)\n",
            ")\n",
            "Epoch 0\n",
            "Train Loss: 0.07781948585994541\n",
            "Validation Loss: 0.2355303904041648\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 1\n",
            "Train Loss: 0.044734375057742\n",
            "Validation Loss: 0.17950538225471974\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 2\n",
            "Train Loss: 0.034175390191376206\n",
            "Validation Loss: 0.1372361627779901\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 3\n",
            "Train Loss: 0.02608611384872347\n",
            "Validation Loss: 0.09255675294436515\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 4\n",
            "Train Loss: 0.016654929337091742\n",
            "Validation Loss: 0.06677638921886682\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 5\n",
            "Train Loss: 0.01241205916274339\n",
            "Validation Loss: 0.05499968620017171\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 6\n",
            "Train Loss: 0.010294352564960719\n",
            "Validation Loss: 0.049164203585824\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 7\n",
            "Train Loss: 0.009169357312493957\n",
            "Validation Loss: 0.0435595354065299\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 8\n",
            "Train Loss: 0.008364838290726767\n",
            "Validation Loss: 0.04070048514520749\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 9\n",
            "Train Loss: 0.007952852606540545\n",
            "Validation Loss: 0.03939801376312971\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 10\n",
            "Train Loss: 0.007545674868160859\n",
            "Validation Loss: 0.03847230819548713\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 11\n",
            "Train Loss: 0.007142638374352828\n",
            "Validation Loss: 0.03649844778701663\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 12\n",
            "Train Loss: 0.006942947676870972\n",
            "Validation Loss: 0.03497062069363892\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 13\n",
            "Train Loss: 0.006539730323420372\n",
            "Validation Loss: 0.03409587401896715\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 14\n",
            "Train Loss: 0.006375900324201211\n",
            "Validation Loss: 0.03409580633509904\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 15\n",
            "Train Loss: 0.006155590589623898\n",
            "Validation Loss: 0.031755756111815574\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 16\n",
            "Train Loss: 0.005846522649517283\n",
            "Validation Loss: 0.032243020631140096\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 17\n",
            "Train Loss: 0.005757020240998827\n",
            "Validation Loss: 0.03023309964686632\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 18\n",
            "Train Loss: 0.005615294149611145\n",
            "Validation Loss: 0.029317325633019208\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 19\n",
            "Train Loss: 0.005516500724916114\n",
            "Validation Loss: 0.028741663275286555\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 20\n",
            "Train Loss: 0.005410896084504202\n",
            "Validation Loss: 0.027778851709445006\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 21\n",
            "Train Loss: 0.005237182924756781\n",
            "Validation Loss: 0.02725683365948498\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 22\n",
            "Train Loss: 0.005219382469949779\n",
            "Validation Loss: 0.02662775181233883\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 23\n",
            "Train Loss: 0.005055637078721702\n",
            "Validation Loss: 0.026655846233479677\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 24\n",
            "Train Loss: 0.005056207068264484\n",
            "Validation Loss: 0.026319323596544562\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 25\n",
            "Train Loss: 0.004980890558799729\n",
            "Validation Loss: 0.024741466480772942\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 26\n",
            "Train Loss: 0.004834393972996623\n",
            "Validation Loss: 0.024761008191853763\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 27\n",
            "Train Loss: 0.005173409747658298\n",
            "Validation Loss: 0.02505181933287531\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 28\n",
            "Train Loss: 0.004722102221567184\n",
            "Validation Loss: 0.02341089183697477\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 29\n",
            "Train Loss: 0.0045821687669376845\n",
            "Validation Loss: 0.023078275616280734\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 30\n",
            "Train Loss: 0.0045842767832800745\n",
            "Validation Loss: 0.02405672618704557\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 31\n",
            "Train Loss: 0.004459935828926973\n",
            "Validation Loss: 0.023150472462875767\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 32\n",
            "Train Loss: 0.004436895224789623\n",
            "Validation Loss: 0.0220974093163386\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 33\n",
            "Train Loss: 0.0043501315684989095\n",
            "Validation Loss: 0.02183402262162417\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 34\n",
            "Train Loss: 0.004209465127205476\n",
            "Validation Loss: 0.02281108365976252\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 35\n",
            "Train Loss: 0.004165783345233649\n",
            "Validation Loss: 0.020603326784912498\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 36\n",
            "Train Loss: 0.004131536424392834\n",
            "Validation Loss: 0.02047410773811862\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 37\n",
            "Train Loss: 0.0040567181762889955\n",
            "Validation Loss: 0.020212749247439207\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 38\n",
            "Train Loss: 0.003974167975829914\n",
            "Validation Loss: 0.020363329860847445\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 39\n",
            "Train Loss: 0.003988942473079078\n",
            "Validation Loss: 0.02001597254537046\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 40\n",
            "Train Loss: 0.0038527313270606102\n",
            "Validation Loss: 0.0192568471445702\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 41\n",
            "Train Loss: 0.0039029610273428263\n",
            "Validation Loss: 0.018876236046198754\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 42\n",
            "Train Loss: 0.0038385004556039347\n",
            "Validation Loss: 0.02040017224440817\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 43\n",
            "Train Loss: 0.00376938330125995\n",
            "Validation Loss: 0.018958488795906305\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 44\n",
            "Train Loss: 0.0036595562868751586\n",
            "Validation Loss: 0.019390303008258343\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 45\n",
            "Train Loss: 0.0036242162267444656\n",
            "Validation Loss: 0.01882850511232391\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 46\n",
            "Train Loss: 0.0036028253415133805\n",
            "Validation Loss: 0.018817208961118013\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 47\n",
            "Train Loss: 0.0036230136582162233\n",
            "Validation Loss: 0.01855862419353798\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 48\n",
            "Train Loss: 0.0035477957676630467\n",
            "Validation Loss: 0.018360981307923793\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n",
            "Epoch 49\n",
            "Train Loss: 0.0034917871555080636\n",
            "Validation Loss: 0.01908392648678273\n",
            "Unused Loss: 0\n",
            "###############################################################################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "student = StudentAgent(train_loader, test_loader, learning_rate=1e-3, threshold=1e-3)\n",
        "prova = student.train(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6ea67bf5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model\n",
        "dest = '../checkpoints/'\n",
        "name = 'Behavioral-Cloning-Agent'\n",
        "if not exists(dest): \n",
        "  mkdir(dest)\n",
        "else: \n",
        "    if exists(dest+name.lower()+'.pt'):\n",
        "        remove(dest+name.lower()+'.pt')\n",
        "torch.save(student.policy.state_dict(), dest+name.lower()+'.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "policy net:  Sequential(\n",
            "  (0): Linear(in_features=196, out_features=16, bias=True)\n",
            "  (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU()\n",
            "  (3): Linear(in_features=16, out_features=36, bias=True)\n",
            ")\n",
            "torch.Size([1, 196])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(36,)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "student = StudentAgent(train_loader, test_loader, learning_rate=1e-3, threshold=1e-3)\n",
        "obs = train_loader.dataset.__getitem__(0)[0].float().unsqueeze(0)\n",
        "print(obs.shape)\n",
        "student.policy.eval()  # Set the model to evaluation mode\n",
        "student.policy(obs).squeeze().detach().numpy().shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
